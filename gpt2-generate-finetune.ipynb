{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7LoMj4GA4n_"
   },
   "source": [
    "#  GPT-2 Generation and Fine-Tuning\n",
    "\n",
    "This notebook explores GPT-2 (Generative Pretrained Transformer-2) from OpenAI. Read more about it [here](https://openai.com/blog/better-language-models/).\n",
    "\n",
    "Activities include:\n",
    "\n",
    "0. Setup\n",
    "1. Generate samples from pre-trained gpt-2 model\n",
    "2. Fine-tune gpt-2 on text of your choosing. \n",
    "\n",
    "Adapted by Robert Twomey (rtwomey@unl.edu) for Machine Learning for the Arts SP22 from this [google colab](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce) by [Max Woolf](http://minimaxir.com). See his repo [gpt-2-simple](https://github.com/minimaxir/gpt-2-simple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run once to install the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q gpt-2-simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "restart the kernel and run the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KBkpRgBCBS2_"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import gpt_2_simple as gpt2\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bj2IJLHP3KwE"
   },
   "source": [
    "## GPU\n",
    "\n",
    "Colaboratory uses either a Nvidia T4 GPU or an Nvidia K80 GPU. The T4 is slightly faster than the old K80 for training GPT-2, and has more memory allowing you to train the larger GPT-2 models and generate more text.\n",
    "\n",
    "You can verify which GPU is active by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sUmTooTW3osf",
    "outputId": "c9fcfa4f-277d-4b3e-8974-373066dc157b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 21 17:51:30 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.63.01    Driver Version: 470.63.01    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    28W / 250W |      0MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note the memory usage (0MiB / 32510MiB) for the Tesla V100.\n",
    "You can re-rerun the above cell to see what memory your code/models are using during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wXB05bPDYxS"
   },
   "source": [
    "## Downloading GPT-2\n",
    "\n",
    "If you're retraining a model on new text, you need to download the GPT-2 model first. \n",
    "\n",
    "There are four released sizes of GPT-2:\n",
    "\n",
    "* `124M` (default): the \"small\" model, 500MB on disk.\n",
    "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
    "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
    "* `1558M`: the \"extra large\", true model. Will not work if a K80/P4 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
    "\n",
    "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below.\n",
    "\n",
    "The next cell downloads it from Google Cloud Storage and saves it in the the current working directory at `/models/<model_name>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P8wSlgXoDPCR",
    "outputId": "10fc0d7c-d18f-4e11-a2af-bfade8b537eb"
   },
   "outputs": [],
   "source": [
    "model_name = \"355M\"  # largest model we can fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run once to download the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.05Mit [00:00, 339Mit/s]                                                      \n",
      "Fetching encoder.json: 1.05Mit [00:00, 4.19Mit/s]                                                   \n",
      "Fetching hparams.json: 1.05Mit [00:00, 954Mit/s]                                                    \n",
      "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:20, 69.5Mit/s]                                 \n",
      "Fetching model.ckpt.index: 1.05Mit [00:00, 817Mit/s]                                                \n",
      "Fetching model.ckpt.meta: 1.05Mit [00:00, 4.22Mit/s]                                                \n",
      "Fetching vocab.bpe: 1.05Mit [00:00, 5.41Mit/s]                                                      \n"
     ]
    }
   ],
   "source": [
    "gpt2.download_gpt2(model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQAN3M6RT7Kj"
   },
   "source": [
    "# 1. Generate Text From The Pretrained Model\n",
    "\n",
    "If you want to generate text from the pretrained model pass `model_name` to `gpt2.load_gpt2()` and `gpt2.generate()`. (This is currently the only way to generate text from the 774M or 1558M models with this notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "BAe4NpKNUj2C",
    "outputId": "b09bfe1d-2ff8-4b8a-fffb-273d28d5d4ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 17:52:06.561981: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-21 17:52:07.745359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30979 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model models/355M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/355M/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "sess = gpt2.start_tf_sess()\n",
    "\n",
    "gpt2.load_gpt2(sess, model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from the model\n",
    "The follow cell samples from gpt-2, using the provided prefix (seed) and other parameters. It starts the TF session and generates the samples.\n",
    "\n",
    "Try changing the parameters below to change the output: \n",
    "- `prefix` is the prompt. This will be the starting string/seed for your generation. Use your own text. \n",
    "- `temperature` sets the variability/randomness of the output. Range 0.0-1.0\n",
    "- `length` sets the lenght of output (in tokens). max is 1024.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 797
    },
    "id": "-xInIZKaU104",
    "outputId": "56348e28-7d08-45e3-c859-f26c0efd066d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The universe is a mysterious place full of possibilities, and it's these possibilities that are sometimes lost in the vastness of time. It's hard to grasp the depths of something so vast, and sometimes, that's all you can get. In the last two years, I've been lucky enough to have seen an incredible amount of stories about the concept of time travel, and I've noticed a common thread: they're all stories that deal with the notion of time travel. These stories are all about time travel being possible, but also about\n",
      "====================\n",
      "The universe is a mysterious place full of mysterious things. How did we get here? How did we get to this place? How can we keep going?\n",
      "\n",
      "My friend Eric is a scientist who has been working with me for a while. He has developed a method of measuring the length of time it takes for a single photon to travel through a closed loop and then through a closed loop to the outside world. He has called this process the \"inverse Compton scattering\" method. He has found that a photon travels through the universe at\n",
      "====================\n",
      "The universe is a mysterious place full of mystery. I think it's pretty clear that the universe is not a flat disk but a fluid nebula. It is a complex and self-moving space-time. So, there are a lot of things that can happen in the universe and that's what I'm interested in.\n",
      "\n",
      "It's interesting that you say that. If you look at the universe, there are a lot of things that are unknown. We don't understand everything. I think that's a good thing. We don\n",
      "====================\n",
      "The universe is a mysterious place full of wonders. Our place in the universe is as important as our place in the world. We are all in the same boat.\n",
      "\n",
      "If you are in a situation where you need to leave your home, and you don't want to leave your home, you will need to be prepared. We know that in the long run, our home is not going to be able to withstand the onslaught of life. If you are an individual who has a home, and your home is not in a safe place\n",
      "====================\n",
      "The universe is a mysterious place full of mysterious things. Like, weird things. And so it's no surprise that the universe is filled with weird things, including the likes of Star Wars, Star Trek, Ghostbusters, and Harry Potter.\n",
      "\n",
      "This is where the Star Wars and Star Trek theories come in.\n",
      "\n",
      "Advertisement\n",
      "\n",
      "The Star Wars and Star Trek theories are that the universe is one big movie made by a single director, and that it's all about the universe ending, and that the universe is all about the events that\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(\n",
    "    sess,\n",
    "    model_name=model_name,\n",
    "    prefix=\"The universe is a mysterious place full of\",\n",
    "    length=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    nsamples=5,\n",
    "    batch_size=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activities\n",
    "- try varying the prefix. \n",
    "  - what length of prefix works best with the given model? \n",
    "  - how does the choice of prefix change the format/form of the output.\n",
    "- try varying the temperature.\n",
    "- try loading the different sized models (124M, 355M, 774M, 1558M) and generate text without changing the other parameters. \n",
    "  - Do you notice any qualitative differences in the output? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fine-Tuning GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already generated with gpt2, you need to reset the tf graph and gpt2 session. Otherwise, we create a new one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeXshJM-Cuaf",
    "outputId": "a3c75caa-917b-4818-ca2d-d78610d8b6f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 17:54:03.166347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30979 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"355M\" # same model as selected above\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# check if sess exists (e.g. if we ran section 1 above)\n",
    "var_exists = \"sess\" in locals() or \"sess\" in globals()\n",
    "\n",
    "if not var_exists:\n",
    "    sess = gpt2.start_tf_sess()\n",
    "else:\n",
    "    sess = gpt2.reset_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeeSKtNWUedE"
   },
   "source": [
    "## Upload a text file\n",
    "For this, we will use a text file you provide to finetune (continue training) GPT-2. You can use any plain text (.txt) file. \n",
    "\n",
    "Simply drag and dropy our text file into the file browser at left. \n",
    "\n",
    "Once you have uploaded your file, update the file name in the cell below, then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6OFnPCLADfll"
   },
   "outputs": [],
   "source": [
    "file_name = \"recipes.txt\"  # your file here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdpZQXknFNY3"
   },
   "source": [
    "## Run the finetuning\n",
    "\n",
    "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
    "\n",
    "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every `save_every` steps (can be changed) and when the cell is stopped.\n",
    "\n",
    "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them. If your input text is smaller, training might proceed more quickly.\n",
    "\n",
    "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
    "\n",
    "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
    "* **`sample_every`**: Number of steps to print example output\n",
    "* **`print_every`**: Number of steps to print training progress.\n",
    "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
    "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
    "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For larger models, the recommended finetune() parameters are:\n",
      "\tuse_memory_saving_gradients = True\n",
      "\tonly_train_transformer_layers = True\n",
      "\taccumulate_gradients = 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 17:54:48.772074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30979 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint models/355M/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models/355M/model.ckpt\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 24094 tokens\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 | 26.90] loss=2.13 avg=2.13\n",
      "[20 | 46.70] loss=1.90 avg=2.01\n",
      "======== SAMPLE 1 ========\n",
      " to the same point.\n",
      "\n",
      "I'm going to come back to this topic at the end of this episode (although it's important to note I will be reviewing an older episode of The Simpsons that was originally broadcast more than 30 years ago)… however, if you've gotten this far then you're going to appreciate the context of what I'm going to talk about. It will hopefully provide some insight into why I believe in Homer's belief in Santa Clause and why I think I have no idea what Christmas is.\n",
      "\n",
      "I think it's important to clarify what this concept means today because it is a relatively new concept to us.\n",
      "\n",
      "Back when I was a kid, I remember going to my first school reunion with my best friend's parents and saying to them, \"Why are you teaching us to read? Who are you going to give an education to?\" We got to tell you, I think the answer has to do with Santa Clause…\n",
      "\n",
      "We all know that Santa Clause is one of the oldest Christian myths that existed during the time period! I remember sitting around with my family and discussing it with our mom, watching our dad and little brother play Santa and they both laughed uncontrollably and said, \"What the hell are you talking about? Santa Clause?\"\n",
      "\n",
      "The reason Santa is so important to our belief system is because of both the fact that it is the oldest Christian myth, it is based on what God told God to teach us, and it literally gives us what we want in life!\n",
      "\n",
      "So, we all understand that Santa Clause has something to do with Santa Claus… but how did we get it? What is so special about Santa Clause?\n",
      "\n",
      "Here's the deal that I want to go right past the obvious part regarding the myth itself to show you the next logical thing to believe:\n",
      "\n",
      "In Christianity, Santa Claus is the perfect example of a saint by Jesus' standards because he does the only thing he can do to improve the lives and souls of ordinary people. What about the other saints of Christianity, the other saints of Judaism, the other saints of Hinduism and Buddhism? If Christmas is supposed to bring the good news of peace through giving out presents or having peace through giving out presents, then it doesn't make sense to be the perfect example of a saint by Jesus' standard. It doesn't make sense!\n",
      "\n",
      "So, my first question becomes, what is the best Christian saint that Jesus could have chosen to make the perfect match for Christmas? The fact that I already have a good answer to that question gives me a clue when I see something that actually has Jesus on it! Jesus, who always does \"the best thing that he can\" is the perfect match!\n",
      "\n",
      "So, here's my question again, Santa Claus comes into our mind because Jesus taught us that Santa Claus \"is a man-made object to serve as the perfect substitute for a god\".\n",
      "\n",
      "This means: He isn't a holy and infallible being. He isn't the one giving gifts every day that we deserve! (And to be honest, he is definitely not.) For a saint, there are 3 steps to the perfect match: one is to make a saint, a second is to make a saint for a purpose/goal, and a third is to make a saint with his own personal set of qualities.\n",
      "\n",
      "The perfect match comes down to what he's doing for a purpose (i.e. Christmas). In other words, Santa Claus is a \"perfectly good\" saint because he's giving gifts that are necessary for a holy person to bless his family. He is a \"perfectally good\" saint because he's helping us to find a perfect match for ourselves!\n",
      "\n",
      "So, we can say, because Jesus had that perfect match for Christmas: he wasn't just giving presents that anyone could be happy with. He was actually giving gifts that were perfect for his family because he was giving them to God.\n",
      "\n",
      "Now, what's the purpose of a Santa? One of the key questions that we ask ourselves from the first Easter prayer before our Holy Spirit arrives is: What is the purpose of Christmas? Is it an important time of year for us? Is it important for the Church? Are we trying to bring about change on earth (saved earth) or bring about salvation on earth (uncreated earth)? If you're a Saint, you're going to answer those question by asking the question, is it important for God to bless Christmas?\n",
      "\n",
      "If you answered yes to every aspect of that question then Santa Claus must be a perfect match because Santa Claus is an exact match of Jesus (saved from sin). Since Santa Claus is not perfect, however, God doesn't want him.\n",
      "\n",
      "So, when you come to see what perfect matches Jesus has on Christmas, you see something that Jesus is very proud of…\n",
      "\n",
      "Santa Claus is the perfect example of Jesus giving gifts that were necessary for a saint to bless his family… and Jesus knew something was \"very special\" about Christmas!\n",
      "\n",
      "\n",
      "[30 | 79.65] loss=1.60 avg=1.88\n",
      "[40 | 99.45] loss=1.65 avg=1.82\n",
      "[50 | 119.24] loss=1.25 avg=1.70\n",
      "Saving checkpoint/run1/model-50\n"
     ]
    }
   ],
   "source": [
    "gpt2.finetune(\n",
    "    sess,\n",
    "    dataset=file_name,\n",
    "    model_name=model_name,\n",
    "    steps=50,\n",
    "    restore_from=\"fresh\",  # change to 'latest' to resume\n",
    "    run_name=\"run1\",\n",
    "    print_every=10,\n",
    "    learning_rate=1e-5,\n",
    "    sample_every=25,\n",
    "    save_every=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXSuTNERaw6K"
   },
   "source": [
    "## Notes on finetuning\n",
    "\n",
    "Keep an eye on the loss, and how quickly it is dropping. A too-rapid drop in loss could be a sign of overfitting, and a learning rate (lr) that is too high. \n",
    "\n",
    "After the model is trained, you can download the checkpoint folder to save your work. Training checkpoints are saved to `checkpoint/run1` (or whatever you chose for the run name above).\n",
    "\n",
    "You can compress it to a rar file and download that. Ask the instructor how.\n",
    "\n",
    "You're done! Feel free to go to the Generate Text From The Trained Model section to generate text based on your retrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune some more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already generated with gpt2, you need to reset the tf graph and gpt2 session. Otherwise, we create a new one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeXshJM-Cuaf",
    "outputId": "a3c75caa-917b-4818-ca2d-d78610d8b6f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-21 17:59:52.730023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30979 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"355M\" # same model as selected above\n",
    "\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# check if sess exists (e.g. if we ran section 1 above)\n",
    "var_exists = \"sess\" in locals() or \"sess\" in globals()\n",
    "\n",
    "if not var_exists:\n",
    "    sess = gpt2.start_tf_sess()\n",
    "else:\n",
    "    sess = gpt2.reset_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fine-tune some more, run the following. Be sure to increase the number of steps (if it was `500` before, change to `1000` to train for 500 more. the number is cumulative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For larger models, the recommended finetune() parameters are:\n",
      "\tuse_memory_saving_gradients = True\n",
      "\tonly_train_transformer_layers = True\n",
      "\taccumulate_gradients = 1\n",
      "\n",
      "Loading checkpoint checkpoint/run1/model-50\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-50\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 24094 tokens\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60 | 28.76] loss=1.24 avg=1.24\n",
      "[70 | 48.69] loss=1.06 avg=1.15\n",
      "[80 | 68.62] loss=1.06 avg=1.12\n",
      "[90 | 88.54] loss=0.90 avg=1.06\n",
      "[100 | 108.47] loss=0.74 avg=1.00\n",
      "Saving checkpoint/run1/model-100\n",
      "======== SAMPLE 1 ========\n",
      " lightly, add half of the flour mixture. Roll the dough into a 12-in. rope. Place in a greased bowl; cover and let rise in a warm place until double in bulk, about 45 minutes. Spray a 10-in. springform pan 2 or 3/4 in. deep with cooking spray. Prepare filling. Combine flour, butter, sugar, salt and vanilla. Roll dough into a 12-in. rope; tie off and chill. Place 1/4 in. apart on prepared pan. Cover with filling; bake until golden brown, 22-25 minutes.\n",
      "\n",
      "Beat egg and 2 Tbsp. canola oil in a small bowl until butter melts. Gradually beat in remaining 2 Tbsp. canola oil. Spread butter-drenched dough into 1-tablespoon glaze; allow to rest 10 minutes. Top with remaining dough; sprinkle with remaining canola oil.\n",
      "\n",
      "Meanwhile, prepare filling. In a small saucepan over medium-high heat, cook eggs until heated and cracked, 3-5 minutes. Remove from the heat. In a large bowl, beat cream cheese, sugar, remaining 2 eggs, cinnamon, salt and vanilla. Stir in enough remaining flour to form a soft dough (dough will be very soft). Turn onto a floured surface; knead until smooth and elastic, 6-8 minutes. Place 2- to 3-in. apart in 12-in. floured rounds. Secure with kitchen twine. Refrigerate until firm, 9-12 hours or until firm enough to roll.\n",
      "\n",
      "In a large bowl, beat 1 cup sugar and molasses at a time, scraping bowl often. Add orange zest; beat until blended. Stir in coffee; chill.\n",
      "\n",
      "Preheat oven to 325°. Brush orange slices with confectioners' sugar.\n",
      "\n",
      "Preheat oven to 375°.\n",
      "\n",
      "In a small bowl, combine sugar and molasses; beat on low speed 3-5 minutes or until smooth. Stir in orange zest. Pour over slices of orange. Spread 1/4 of the batter on top.\n",
      "\n",
      "To make orange crèmant, combine cake ingredients in a small bowl. Add enough rum to coat to the last 2/3 of the way. Stir until smooth. (Drop amount of rum into the center of each crèmant. Top with the remaining rum and remaining cake.)\n",
      "\n",
      "To make orange crèmant, combine rum with orange zest. Spread rum over top of oranges. Top with remaining cake.\n",
      "\n",
      "To make orange cake batter, beat cake ingredients at room temperature for 2 minutes. Whisk in almond milk until smooth. Gradually beat in rum. Drizzle overcakes.\n",
      "When orange cake is nearly set, brush 1/2 cup flour over top. Spread a thin layer of batter, flattening to 1/2-in. thickness. Refrigerate 2 hours or until firm to the touch.\n",
      "Garnish pineapple with fresh orange slices.\n",
      "Orange Cake\n",
      "Ingredients\n",
      "   1 package (1/4 ounce) active dry yeast, divided\n",
      "   1 package (7-1/4 to 2-3/4 ounces) instant or dried rosemary, divided\n",
      "Stir peppercorns, 1/2 teaspoon salt and 1/4 teaspoon ground cinnamon into warm warm water until smooth. Cover and let stand at room temperature, covered, 30 minutes.\n",
      "   7 tablespoons (1/4 ounce) butter, softened\n",
      "Preheat oven to 375°. Heat oil in a large skillet over medium-high heat. Add onion and cook, stirring often, until translucent, 6-8 minutes. Add peppers, 1 to 2 minutes apart; stir-fry 4 minutes longer or until vegetables are tender. Stir in onion mixture and remaining rosemary.\n",
      "While vegetables are cooking, combine flour, salt, poppy seeds and remaining yeast in a small bowl. Add warm potatoes, potatoes and parsnips; toss to coat. Cook and stir 2 minutes. Stir in butter and remaining rosemary. Mix well; season to taste.\n",
      "Cooks should use enough potato mixture to make 12 servings.\n",
      "Remove skin from potatoes, cut into 1/2-in. slices. In a small bowl, combine potatoes, remaining yeast mixture; season to taste with salt and poppy seeds. Pour over potatoes.\n",
      "In a large bowl, combine flour, onion powder, cinnamon and sugar; toss gently to coat ingredients in all ingredients. Cut into 1-in. wedges.\n",
      "Preheat oven to 375°.\n",
      "On a lightly floured surface, roll up 6-in. rectangle of cheese. Cut with a circular motion, starting at 1:1 with a sharp knife. If you need more clearance, press the edge through the center.\n",
      "Pour in 1 cup rum. Repeat 5 times. Bake until cheese is bubbly, 30 to 35 minutes. Cool on a wire rack.\n",
      "For topping, with a knife, cut a log about the size of a\n",
      "\n",
      "[110 | 150.17] loss=0.66 avg=0.94\n",
      "[120 | 170.10] loss=0.80 avg=0.92\n",
      "[130 | 190.02] loss=0.62 avg=0.88\n",
      "[140 | 209.94] loss=0.46 avg=0.83\n",
      "[150 | 229.90] loss=0.48 avg=0.79\n",
      "[160 | 249.82] loss=0.42 avg=0.76\n",
      "[170 | 269.76] loss=0.31 avg=0.72\n",
      "[180 | 289.67] loss=0.46 avg=0.70\n",
      "[190 | 309.51] loss=0.30 avg=0.67\n",
      "[200 | 329.31] loss=0.24 avg=0.64\n",
      "Saving checkpoint/run1/model-200\n",
      "WARNING:tensorflow:From /util/opt/anaconda/deployed-conda-envs/packages/tensorflow-gpu/envs/tensorflow-gpu-2.6.0-py39/lib/python3.9/site-packages/tensorflow/python/training/saver.py:969: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "======== SAMPLE 1 ========\n",
      " room. \n",
      "9. In a small bowl, combine 2/3 cup flour and ground cinnamon. Sprinkle additional flour on a floured surface. Roll into 1-in. balls. Roll up jelly-roll style, starting with a short side; pinch ends to seal. Place on greased glaze plates. Repeat with remaining flour, filling and cinnamon. Cover and let rise until doubled, about 45 minutes. ",
      "For batter, cream cream cheese and butter in a large bowl until creamy. Add confectioners' sugar; beat until fluffy. Stir in vanilla. Fold in orange zest. If desired, top with additional orange zest. Spread batter just before decorating.\n",
      "\n",
      "Cake Batter\n",
      "\n",
      "Ingredients\n",
      " pancakes 10 cups all-purpose flour, divided\n",
      "  1/2 cup sugar\n",
      " syrup 3 tablespoons all-purpose flour plus more for dusting the baking sheets\n",
      " white vinegar 1 tablespoon cold water 1 tablespoon all-purpose flour 1/4 teaspoon salt 1 large egg white, room temperature, beaten 5 cups sliced peeled tart apples, toasted 3 cups chopped raspberries Method Combine the flour, sugar and 2 cups water in a small saucepan; bring to a boil. Cook and stir for 2 minutes or until mixture resembles coarse crumbs. Add the egg white and vanilla; cook and stir for 2 more minutes or until thickened. Remove from the heat; stir in enough fruit to allow batter to come together. Cool completely. Punch down the tart pan. ",
      "In a large bowl, beat the flour, sugar and remaining egg white until smooth. Add the yeast; beat until smooth. Stir in enough fruit to make a soft dough. ",
      "Turn out onto a floured surface; divide into thirds. Pick up the uneven pieces and phase them repeatedly with a thin amount of dough. Make 4-in. spreads*. Repeat with remaining dough. Cut 4 slits in the center of each spread portion of dough; use a small knife to tie a knot. Place 2 in. apart from each other on the floured surface. Repeat with remaining dough, picking up the loaves with a knife. Make 1-in. circles with a knife; tie a knot at the middle. Punch down dough. Cover and let rise in a warm place until doubled, about 1 hour. ",
      "Bake at 350° for 10-12 minutes. Remove to wire racks. When rolls are light golden brown, remove from pan to a wire rack to cool. Repeat with the remaining dough.\n",
      "\n",
      "Ingredients\n",
      " pancakes 1 package (12 ounces) solid pineapple, peeled and deseeded**\n",
      " syrup 2 teaspoons packed brown sugar\n",
      " bananas, sliced 3 in. long\n",
      " glue Gun brand or household glue\n",
      " orange zest, toasted red food coloring\n",
      " yellow food coloring\n",
      " red food coloring\n",
      " white glue\n",
      " store cupcakes blocks, alternately, with strawberry cups, cookie sheets and frosting stacks Peanut butter cups, for stacking stacks\n",
      " cookie sheet Instructions Preheat oven to 350°. Line bottoms of two greased 9-in. round baking pans with parchment; grease parchment. In a small bowl, combine pineapple slices, brown sugar and zest. Drizzle over batter; let dry for 2 minutes. In a double boiler, over medium-high heat, stir pineapple slices and brown sugar until completely combined. Remove from heat. In a small bowl, combine confectioners' sugar, apples, zest and orange juice. Refrigerate until set, at least 2 hours. At room temperature, prepare stacks of cookies. For stacks of 1.25-1.5 pounds, use any type of cookie sheet you would like, such as 22x8 baking pan, cake panel or skillet. For stacks of 2.75-3.0 pounds, use parchment-bounding pan. For stacks of 5.0-5.75 pounds, use double-sided pan. For stacks of 6.0-6.5 pounds, use unsprinted baking sheets. For stacks of 6.75-8.25 pounds, use parchment-bounding pan. For stacks of 10.5-12.5 pounds, use tipi or star baking sheets. Spoon dough onto baking sheets. Bake until edges begin to brown, 8-10 minutes. Cool on pans 2 minutes. Remove to wire racks to dry. If desired, decorate layers with strawberry cups. If desired, use leftover cake layers. If desired, decorate each cup with a single pecan.\n",
      "Spray counterbake with cooking spray if bowls are to be used exclusively for bowlful mixing.\n",
      "Batter:\n",
      "115 calories; 6 g fat; 6 g sat. fat; 2 g unsecreant linoleate g.% solids%; 311 mg c.% chol. mg\n",
      "If I made your Bake with Milk recipe, what would it taste like? yummy!\n",
      "This recipe\n",
      "\n",
      "[210 | 363.71] loss=0.16 avg=0.61\n",
      "[220 | 383.49] loss=0.18 avg=0.58\n",
      "[230 | 403.28] loss=0.16 avg=0.55\n",
      "[240 | 423.19] loss=0.15 avg=0.53\n",
      "[250 | 443.13] loss=0.15 avg=0.51\n",
      "[260 | 463.07] loss=0.18 avg=0.49\n",
      "[270 | 482.98] loss=0.16 avg=0.48\n",
      "[280 | 502.90] loss=0.13 avg=0.46\n",
      "[290 | 522.82] loss=0.10 avg=0.44\n",
      "[300 | 542.75] loss=0.18 avg=0.43\n",
      "Saving checkpoint/run1/model-300\n",
      "======== SAMPLE 1 ========\n",
      " butter, 2 tablespoons olive oil, honey and cinnamon to taste\n",
      "\n",
      "Toppings\n",
      " slivered onto brioche layers\n",
      "\n",
      "Directions\n",
      " Truss and cut dough into ¾-inch pieces. Place in a greased 10-in. springform pan, cut side up. Combine 2 teaspoons brown sugar and 1 teaspoon salt; press down to mix. ",
      "In a large bowl, beat butter and sugar until fluffy, 6-8 minutes. Beat in egg, enough flour to slightly incorporate egg yolk to create a soft dough. Drop batter in; bake until light golden brown, 20-25 minutes. Cool on a wire rack. If desired, serve with whipped cream and fresh strawberries.\n",
      "\n",
      "Ingredients\n",
      "\t1 cup all-purpose flour\n",
      "\t1/3 cup packed brown sugar\n",
      "\t1/3 cup first-form honey\n",
      "\t1/3 cup unsweetened applesauce\n",
      "\t2 large eggs, room temperature\n",
      "\t1 teaspoon baking soda\n",
      "\t1 tablespoon canola or peanut oil\n",
      "\t1 teaspoon vanilla extract\n",
      "\t2 cups all-purpose flour\n",
      "\t1 cup cold mashed potatoes\n",
      "\u00141 cup chopped pecans\n",
      "\t1 cup chopped dates\n",
      "\t1 cup chopped walnuts\n",
      "\t1 cup chopped figs\n",
      "\t1 cup sliced fresh or frozen nonflesied apples\n",
      "\t1 cup chopped rhubarb\n",
      "\t1 cup chopped pepitas\n",
      "\t1 cup chopped sunflower seeds\n",
      "\t1 cup chopped coconut milk\n",
      "\t1 teaspoon vanilla extract\n",
      "\t1/2 teaspoon baking soda\n",
      "\t1/4 teaspoon salt\n",
      "\t1 cup sweetened shredded coconut\n",
      "\n",
      "Directions\n",
      "\tCombine first 4 ingredients in a large bowl; add 2 packages dried cranberries and 2 cups flour. Chill (do not freeze). ",
      "Preheat oven to 350°. Cut fruit into 1/2-in. slices. Place 2 in. apart on greased baking sheets. Bake until edges begin to brown, 6-8 minutes. Cool 2 minutes before removing from pans to wire racks. Combine cranberries and remaining flour; cut into chunks. Stir in applesauce, honey and oil. Spread bakers' trimmings on fruit. Cover loosely to prevent drying. Refrigerate stored fruit 1 hour or until firm enough to cut.\n",
      "EDITOR’S NOTE:\n",
      "We use honey in this recipe. Honey, when analyzed correctly, gives a lemon-citrus flavor.\n",
      "\n",
      "Ingredients\n",
      "\t2-1/4 cups butter, softened\n",
      "\t4 cups sugar\n",
      "\t1-1/4 cups packed brown sugar\n",
      "\t1 large egg, room temperature\n",
      "\t2 tablespoons light corn syrup\n",
      "\t2 teaspoons lemon extract\n",
      "\t3-1/4 cups all-purpose flour\n",
      "\t2-1/4 cups chopped walnuts\n",
      "\t1-1/2 teaspoons ground cinnamon\n",
      "\t4 teaspoons baking soda\n",
      "\t1 teaspoon salt\n",
      "\tglaze:\n",
      "\t1/4 cup sweetened condensed milk\n",
      "\t1/3 cup packed brown sugar\n",
      "\t1/4 cup coarse cornstarch\n",
      "\t4 drops red food coloring, optional\n",
      "\t1/3 teaspoon vanilla extract\n",
      "\n",
      "Directions\n",
      "\tIn a large bowl, cream butter and sugars until light and fluffy, 5-7 minutes. Add egg, 1 at a time, beating well after each addition. Beat in cornstarch, extracts and lemon juice. Combine the flour, nuts and salt; gradually add to creamed mixture. Chill until time for coating allows, about one-and-a-half hours. ",
      "Spray a roasting pan with cooking spray. ",
      "In a large bowl, beat milk, oil, corn syrup, lemon juice, egg, cornstarch and enough remaining flour to form a soft dough (dough will be sticky). Cut into 1/4-in. slices. ",
      "Pinch off dough and chill until needed, 2 hours or until firm. ",
      "In a large bowl, roll out dough to 1/4-in. thickness. Cut into 1/4-in. slices. Place 2 in. apart on prepared rolls. Repeat with remaining dough and rolls. Cover and let rise in a warm place until doubled, about 45 minutes. Preheat oven to 400°. ",
      "Bake 20 minutes or until golden brown. Remove from pans to wire racks to cool. Combine icing ingredients; drizzle over loaves.\n",
      "\n",
      "Ingredients\n",
      "\t1-1/4 cups butter, softened\n",
      "\t3/4 cup sugar\n",
      "\t2 large eggs, room temperature\n",
      "\t2 tablespoons lemon juice\n",
      "\t1 tablespoon lemon zest\n",
      "\t2-1/4 cups all-purpose flour\n",
      "\t1-1/2 teaspoons baking powder\n",
      "\t1/2 teaspoon salt\n",
      "\t1/4 cup cold milk\n",
      "\t1 tablespoon pure vanilla extract\n",
      "\tegg wash:\n",
      "\t2 packages (1/4 ounce) unflavored gelatin\n",
      "\t1 tablespoon cold water\n",
      "\tOptional: Tarter a piece of fresh strawberries\n",
      "\n",
      "[310 | 580.86] loss=0.09 avg=0.42\n",
      "[320 | 600.78] loss=0.09 avg=0.40\n",
      "[330 | 620.72] loss=0.08 avg=0.39\n",
      "[340 | 640.64] loss=0.12 avg=0.38\n",
      "[350 | 660.57] loss=0.07 avg=0.37\n",
      "[360 | 680.50] loss=0.07 avg=0.35\n",
      "[370 | 700.42] loss=0.07 avg=0.34\n",
      "[380 | 720.34] loss=0.08 avg=0.33\n",
      "[390 | 740.26] loss=0.06 avg=0.33\n",
      "[400 | 760.18] loss=0.08 avg=0.32\n",
      "Saving checkpoint/run1/model-400\n",
      "======== SAMPLE 1 ========\n",
      "\n",
      "\n",
      "Ingredients\n",
      "\t2 cups flour\n",
      "\t1/3 cup butter, cubed\n",
      "\t1/3 cup honey\n",
      "\t2 teaspoons salt\n",
      "\t1 large egg, room temperature\n",
      "\t1 cup 2% milk\n",
      "\t2 tablespoons 2% vegetable shortening\n",
      "\t1 tablespoon baking soda\n",
      "\t1 tablespoon cedar-sugar molasses\n",
      "\tfilling:\n",
      "\t1/2 cup 2% milk\n",
      "\t2 cups bread or pastries\n",
      "\t1 cup muffins\n",
      "\t1 cup unmuffed pecan halves\n",
      "\t1 cup sunflower seeds\n",
      "\t2 cups chopped pecans\n",
      "\t1 large apple, peeled and grated\n",
      "\t1/2 cup pineapples, kernels removed\n",
      "\t1/4 cup vanilla extract\n",
      "\n",
      "Directions\n",
      "\tPreheat oven to 350°. Coat a greased 13x9-in. baking dish with cooking spray; set aside. ",
      "In a large bowl, cream the butter, honey, salt, egg and 2 cups bread ingredients until fluffy. Add to a large bowl; gradually add remaining ingredients and mix well. Pour into prepared baking dish. Bake until a toothpick inserted in center comes out clean, 20-25 minutes. Cool 10 minutes before removing from pan to a wire rack. ",
      "For filling, in a small bowl, mix pecans and apple pieces. In a large bowl, whisk milk, flour and shortening until smooth. Add to creamed mixture; stir just until moistened. Spread filling to within 1/2 in. of edges. Top with remaining pecans and top with breadskins (optional). Bake until a toothpick inserted in center comes out clean, 20-25 minutes. Cool on a wire rack. ",
      "In a small bowl, beat 2% milk and sugar until smooth. Drizzle over cake; let stand 1 minute. To serve, spoon 1 in. filling over cake, spreading to coat. Repeat with remaining filling and top with sandwich shells. Store sandwiches fresh at room temperature.\n",
      "\n",
      "Ingredients\n",
      "\t1/2 cup butter, softened\n",
      "\t1 cup sugar\n",
      "\t1/2 cup confectioners' sugar\n",
      "\t1 large egg, room temperature\n",
      "\t1 teaspoon gr. vanilla extract\n",
      "\t2-1/2 cups all-purpose flour\n",
      "\t1/2 teaspoon baking soda\n",
      "\t1/2 teaspoon salt\n",
      "\t1 cup coffee grounds\n",
      "\t1 cup cold brewed coffee\n",
      "\tCoffee:\n",
      "\t4 cups whole milk\n",
      "\t1 cup buttermilk\n",
      "\t2 tablespoons butter, softened\n",
      "\t1 tablespoon ground cinnamon\n",
      "\t1 teaspoon peppercorns\n",
      "\t1 teaspoon ginger ale\n",
      "\t1 tablespoon water\n",
      "\n",
      "Directions\n",
      "\tIn a large bowl, cream butter and sugar until light and fluffy, 5-7 minutes. Add egg; beat well. Add vanilla; stir just until combined. Add milk, buttermilk, butter and cinnamon; gradually add, beating well after each addition. Stir in peppercorns and ginger. ",
      "Pour into 2 greased and floured 9-in. round baking pans. Bake at 350° until a toothpick inserted in the center comes out clean, 22-25 minutes. Cool for 2 minutes before removing from pans to wire racks to cool completely. ",
      "For coffee grounds, in a small saucepan, combine the cold water and enough whole coffee grounds to reach about 1/3 of a cup's volume. Bring to a boil; cook and stir until smooth. Pour over cake; let stand 1 minute. Serve with cake.\n",
      "\n",
      "Ingredients\n",
      "\t2 cups butter, softened\n",
      "\t1 cup sugar\n",
      "\t1/2 cup packed brown sugar\n",
      "\t3 large eggs, room temperature\n",
      "\t1 teaspoon vanilla extract\n",
      "\t5 cups all-purpose flour\n",
      "\t1-1/2 teaspoons baking powder\n",
      "\t1/2 teaspoon salt\n",
      "\t1/4 cup 2% milk\n",
      "\n",
      "Directions\n",
      "\tIn a large bowl, cream butter and 2 cups sugar until light and fluffy. Beat in eggs and vanilla. Combine flour, baking powder and salt; gradually add to creamed mixture and mix well. Roll dough into a 15x4-in. rectangle; place in a greased bowl and cut into wedges. ",
      "Place 1 in. apart on greased baking sheets. Bake at 350° for 10-12 minutes or until edges begin to brown. Remove from pans to wire racks to cool.\n",
      "\n",
      "Ingredients\n",
      "\t4 cups fresh blackberries\n",
      "\t1-1/2 cups all-purpose flour\n",
      "\t2 teaspoons baking soda\n",
      "\t1 teaspoon ground cinnamon\n",
      "\t2/3 cup sugar\n",
      "\t1 large egg, room temperature\n",
      "\t1/2 cup chopped nuts\n",
      "\t1 teaspoon lemon juice\n",
      "\tMaple Syrup:\n",
      "\t1/2 cup packed brown sugar\n",
      "\t1/2 cup packed cream\n",
      "\t1/4 cup sugar\n",
      "\t1 tablespoon Maple Syrup\n",
      "\n",
      "Directions\n",
      "\tPreheat oven to 350°. In a large\n",
      "\n",
      "[410 | 798.25] loss=0.08 avg=0.31\n",
      "[420 | 818.17] loss=0.06 avg=0.30\n"
     ]
    }
   ],
   "source": [
    "gpt2.finetune(\n",
    "    sess,\n",
    "    dataset=file_name,\n",
    "    model_name=model_name,\n",
    "    steps=400,\n",
    "    restore_from=\"latest\",  # change to 'latest' to resume\n",
    "    run_name=\"run1\",\n",
    "    print_every=10,\n",
    "    learning_rate=1e-5,\n",
    "    sample_every=100,\n",
    "    save_every=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClJwpF_ACONp"
   },
   "source": [
    "# 3. Generate Text From The Finetuned Model\n",
    "\n",
    "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4RNY6RBI9LmL",
    "outputId": "82574eaa-d39a-4665-b611-e5172848da57"
   },
   "outputs": [],
   "source": [
    "# gpt2.generate(sess, run_name='run1') # no prefix, unconditional generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anakin had the high ground this time, his lightsaber out.\n",
      "PALPATINE\n",
      "I told you, you would not defeat Count Dooku\n",
      "with words alone!\n",
      "ANAKIN\n",
      "I know you do, Chancellor.\n",
      "PALPATINE\n",
      "I have relied on the teachings of your\n",
      "Unified Force™ Sages, Anakin. They\n",
      "have trained me in the ways of the dark\n",
      "lightning, and I’ve become them.\n",
      "ANAKIN\n",
      "I wonder what it would be like to learn\n",
      "from you yourself.\n",
      "PALPATINE\n",
      "You will learn much you will, Anakin.\n",
      "You have always known how to act on impulse\n",
      "and lose your temper, but you will learn\n",
      "to control your emotions, to accept\n",
      "whatever comes your way.\n",
      "ANAKIN\n",
      "I can see how it may be difficult.\n",
      "PALPATINE\n",
      "Your faith in your abilities is unwavering\n",
      "I can assure you. But I do hope\n",
      "that over the coming days and\n",
      "weeks you will gain more self-discipline.\n",
      "ANAKIN\n",
      "Thank you, Chancellor.\n",
      "PALPATINE\n",
      "Are you ready, my apprentice?\n",
      "EXT. CORUSCANT - SENATE CHAMBER\n",
      "The WOOKIRETURER delivers the news to the Chancellor\n",
      "and his Council.\n",
      "ANAKIN\n",
      "Yes, sir.\n",
      "INT. LANDING PLATFORM - TREK - NABOO SKIFF\n",
      "The droids bring down the main floor landing gear,\n",
      "clutches and sets in seconds.\n",
      "A wounded OBI-WAN KENOBI sits in the grass, greatly\n",
      "advised by the remaining SEPARATISTS in the craft.\n",
      "A massive WOOKIRETURER walks with the others down the\n",
      "interior lawn of the Senate Complex.\n",
      "91.\n",
      "INT. BAIL ORGANA'S APARTMENT - NABOO SKIFF\n",
      "Bail Organaka lies on his bed, unconscious.\n",
      "Anakin rushes over and comfortes him.\n",
      "ANAKIN\n",
      "It's a droid.\n",
      "BAIL ORGANA\n",
      "Get help from the ship’s medical\n",
      "team’s -\n",
      "ANAKIN\n",
      "We’re going to the infirmary.\n",
      "(sobbing)\n",
      "We’re going to be kids again, someday.\n",
      "BAIL ORGANA\n",
      "(sobbing)\n",
      "Padme gave up hope on the galaxy\n",
      "system. She’s died a thousand deaths\n",
      "discovering alien civilizations far, far\n",
      "away.\n",
      "Anakin hugs Obi-Wan.\n",
      "ANAKIN\n",
      "Save your breath, we’re going to be\n",
      "young again.\n",
      "OBI-WAN KENOBI\n",
      "You’re crazy.\n",
      "ANAKIN\n",
      "Heard them before, sayin?\"\n",
      "OBI-WAN KENOBI\n",
      "Heard what?\n",
      "ANAKIN\n",
      "How the Jedi killed Anakin?\n",
      "OBI-WAN KENOBI\n",
      "Heard what?\n",
      "Anakin nods to himself. He’s aware of the burden\n",
      "that speak softly and carry many, but he’s\n",
      "unaware of the messages they are sending.\n",
      "92.\n",
      "ANAKIN\n",
      "I’m going to be different. I’m going\n",
      "to get my revenge.\n",
      "OBI-WAN KENOBI\n",
      "You are Kyle Katarn’s son. Can you\n",
      "stop thinking about that?\n",
      "ANAKIN\n",
      "I can.\n",
      "OBI-WAN KENOBI\n",
      "You won't. You keep trying.\n",
      "ANAKIN\n",
      "I'm not letting this stop us. We make\n",
      "the rules.\n",
      "OBI-WAN KENOBI\n",
      "But you’re the one who has to break them.\n",
      "ANAKIN\n",
      "I’m putting you on notice. Either you\n",
      "help or I leave.\n",
      "Obi-Wan leaves the bedroom.\n",
      "Obi-Wan looks at Anakin.\n",
      "PALPATINE\n",
      "What are you waiting for, Anakin?\n",
      "Obi-Wan doesn’t answer.\n",
      "PALPATINE\n",
      "He's left me no choice but to end this\n",
      "situation.\n",
      "Obi-Wan stands. He walks to the center of the bedroom.\n",
      "93.\n",
      "He kneels before the fallen Jedi.\n",
      "Ki-Adi-Mundi\n",
      "\n",
      "Rahm Kota\n",
      "\n",
      "Gives Obi-Wan's coat and lightsaber.\n",
      "\n",
      "94.\n",
      "AUGUST BREAK.\n",
      "\n",
      "Anakin and Obi-Wan walk into the Jedi\n",
      "Temple.\n",
      "\n",
      "ANAKIN\n",
      "(standing)\n",
      " Has she been found?\n",
      "OBI-WAN KENOBI\n",
      "\n",
      "Yes, she’s safe.\n",
      "\n",
      "ANAKIN\n",
      "Then it�\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(sess, run_name=\"run1\", prefix=\"Anakin had the high ground this time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oF4-PqF0Fl7R"
   },
   "source": [
    "## Notes\n",
    "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
    "\n",
    "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
    "\n",
    "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
    "\n",
    "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
    "\n",
    "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
    "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
    "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
    "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
    "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
    "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8DKMc0fiej4N",
    "outputId": "490a4648-d973-4675-cf9a-7a48c16fd736"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment suggests that animals make inferences more easily than humans do. What the research team found was that the chances that a shih tzus face would be curved toward the left were 2.4 times greater than those it would be to the right. This result clearly indicated that animals’ understanding of proportion was more primitive than that of logic.\n",
      "\n",
      "Surrogate Mother Experiment\n",
      "Study Conducted by: Harry Harlow and Rosalie Rayner\n",
      "Study Conducted in 1954 at the University of Michigan\n",
      "\n",
      "Experiment Details: The 1954 Surrogate Mother Experiment was one of the most controversial experiments in psychology. The experiment was designed to investigate the influence of early childhood development and the guidance systems of mother and infant.\n",
      "Upon request of a media outlet for a story on the experiment, Jack Kevorkian of ABC News called into his show to talk about the experiment. During the interview Jack revealed that he and his camera crew were there to interview an unethical mother whose 14-month-old son had an adverse reaction to a new toy. The reporter found it unethical to allow the infant to be left to his own devices and to focus on giving his mother all the details about his accident. Although this may appear harsh\n",
      "====================\n",
      "experiment\"\n",
      "This is generally seen for situations involving choice, where individuals are paired against each other in a harsh situation where they are asked to make a difficult decision. The partners always do the same thing each time they face the same situation and the individual who is asking the difficult question usually does not get a true guess as to what the other person will do each time they face the same situation.\n",
      "The researchers began with a group of participants where a single male was the only male in the group and they were asked to play the role of Mr. Right. The experimenters then had the male Mr. Right watch another man play the same role. This time the roles were reversed: the roles were played by female volunteers, who appeared to be in all ways identically endowed. What impressed the experimenters most about the gender disparity was not that the males were the actors but that the females rarely , if ever, requested the risky action(s) the men undertook as the male characters.\n",
      "In a variety of subsequent studies male to male actors were discovered who were neitheraspergers norroids, who willingly played Mr. Right or Shelly Sherman in They're Both You when asked to identify which one was which,\n",
      "====================\n",
      "experiment\" with fifty undergraduate subjects to please. Thirtyone of the subjects were told they were participating in an experiment and participating was defined as coming into a room and showing a member of the opposite sex any personal information. The remaining forty two subjects were not informed they were being studied and, as a result, only partially answered the sexual-identity survey questionnaires. The results of the study showed that men were more likely to have had a female follow them into the room than woman, segregation was the stronger feeling, the participants were most likely unaware of the task gauge.\n",
      "\n",
      "Little Albert Experiment\n",
      "Study Conducted by: John B. Watson and Rosalie Rayner\n",
      "Study Conducted in1939 at Johns Hopkins University\n",
      "\n",
      "Experiment Details: The Little Albert experiment was designed to study the effects that a coloverturning maneuver has on a colt named Albert. The ill-fated experiment was undertaken in 1939 at Johns Hopkins University to mark the centenary of Albert's injury. The cause of the accident is still a subject of debate, though speculation has ranged from disease to runaway train to man-made disaster.\n",
      "Little Albert was partnered with Rosalie, a colt suffering from colicky under-stimulation\n",
      "====================\n",
      "experiment Reports\". Unpublished doctoral dissertation, Arizona State University, 1980. 80 pages. Headlines included: \"Amazing Science Reveals the Basic Element Behind Schlongs.\" The Washington Post. February 6, 1980. Web.<|endoftext|>Subsplicing Human Embryos into Monkey Primate Cousin Chicks Reveals Rodent Genetic Trait\n",
      "Human chompers portal through mouse genome © britz Sample male adult human-coupling experiment contains a critical flaw: human participants were tricked into thinking that the chimpsthey were studying were brothers. Study co-author Andreus Lammers found that when participants were informed that they werehopped between monkeys, their evaluation ofthe chimpsswitched dramatically. When participants were told they were stinkingfellowexperiments took place in a separate building, the evaluations showed nothing different. In a second experiment,the same thing happened when participantswere informed that they were studying chimps in the Charles River Park in Charlotte, North Carolina. Instead ofshopping for puppies in separate cities, visitors shopped  for chicks. A series of eventsfurther complicated the evaluation,. . . showed that those who were told that theexperiments took place\n",
      "====================\n",
      "experiment\") allows an investigator to observe and relate the actions and thoughts of people under study without actually knowing the individuals participating.\n",
      "This Emotion Experiment was done by John Wheelan and his colleagues at the Stanford Prison Experiment from 1965-1968. The team of researchers set out to study John Wilkes Booth , an anti-semite who was killed in a random selection romanearly convicted murderer case. The experimenters set out to determine if there was any psychological condition which made a personunable to harm otherpeople(humans or objects).Â Booth was the target of the study because he �dislike(ed) to be dominated and physically resisted by a group of them.\n",
      "The experiment began by having seventy two prisonersseung side-by-sideand huddled around a large absorption basin that was kept semi-concealed behind a wire. The basin wasfilled with saline ,and thestrate was underneath the basin. The room was dark except forthe sound of hammering against a steel bar every fifteen seconds. The bricks in the basin werefilled with water every fifteenseconds. The tests started withwater and ended withwater. The Basin was Waterfulه ’s Room was the same everywhere. Everyplace in\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "gpt2.generate(\n",
    "    sess, length=250, temperature=1.2, prefix=\"experiment\", nsamples=5, batch_size=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjjEN2Tafhl2"
   },
   "source": [
    "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
    "\n",
    "You can rerun the cells as many times as you want for even more generated texts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Fa6p6arifSL0"
   },
   "outputs": [],
   "source": [
    "gen_file = \"gpt2_gentext_{:%Y%m%d_%H%M%S}.txt\".format(datetime.utcnow())\n",
    "\n",
    "gpt2.generate_to_file(\n",
    "    sess,\n",
    "    destination_path=gen_file,\n",
    "    length=500,\n",
    "    temperature=1.2,\n",
    "    nsamples=100,\n",
    "    batch_size=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-LRex8lfv1g"
   },
   "source": [
    "Download the file by hand in the browser at left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Trained Model Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTa6zf3e_9gV"
   },
   "source": [
    "Uploaded your saved checkpoint and unzip it.\n",
    "\n",
    "The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n",
    "\n",
    "This will reset or start the tensorflow session as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-fxL77nvAMAX",
    "outputId": "8938432a-3b86-4102-f32b-362721ecb897"
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "if not sess:\n",
    "    sess = gpt2.start_tf_sess()\n",
    "else:\n",
    "    sess = gpt2.reset_session(sess)\n",
    "\n",
    "gpt2.load_gpt2(sess, run_name=\"run1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ig-KVgkCDCKD"
   },
   "source": [
    "# Etcetera\n",
    "\n",
    "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIHiVP53FnsX"
   },
   "outputs": [],
   "source": [
    "!kill -9 -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmTXWNUygS5E"
   },
   "source": [
    "# License\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2019 Max Woolf\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "- Max's [blog post](https://minimaxir.com/2019/09/howto-gpt2/) for more information how to use this notebook!\n",
    "- Original repo: [gpt-2-simple](https://github.com/minimaxir/gpt-2-simple) by [Max Woolf](http://minimaxir.com). \n",
    "- Original [google colab](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce) from Max."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Train a GPT-2 Text-Generating Model w/ GPU",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "TensorFlow GPU 2.6 (py39)",
   "language": "python",
   "name": "tensorflow-gpu-2.6-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
